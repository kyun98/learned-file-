{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2326bf7-d55a-4cd4-aa8c-f89a8eb8e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자연어 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d33a10-0f8b-4aae-ab43-92295df77c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6efa5f7-e86c-4074-9199-b9e9d3411a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6871d998-b640-4097-b5e3-08558d62d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize('is it possible distingushing cats and dogs')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29d335-cb48-4399-90c1-1dc2de4d2d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407958a0-1ca6-4229-a238-0afeb709616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422eba6-9317-4e28-9932-1019f8eaae4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb8ff7d-6b9e-480b-9a45-8cf629571050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271d73a-140f-4a51-83a2-68780429bfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77afa45-8952-4546-abce-276a773efecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe6722-d67c-47fc-a02a-d5313dbe8707",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize(string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ccc0b6-d703-47e9-ab44-6ecfe27e4385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be58a10-60ac-46e5-af00-1e71d1b5c862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4623550-9c6d-4c72-82aa-523c3cab3622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2123c584-1403-4e85-84db-b107b185de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e261e-b420-49e9-b3e0-4bd824ea2b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "komoran = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4805ee1-aec7-4ce0-9f86-8144188450f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(komoran.morphs('딥러닝 수업 쉬워요? 어려웡ㅅ?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807cc90-67cd-42c3-888a-9994f20ec0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(komoran.pos('소파 위에 있는 동물이 고양이 인가요? 강아지인가요? 긴가민가요?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e058683a-ed23-40cb-b101-afa8efaae23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b988c-04a6-4e9f-b6f9-1e43b6d646fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "string1=\"my favorite subject is math\"\n",
    "string2=\"my favorite subject is math, english, economic and computer science\"\n",
    "nltk.word_tokenize(string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d05c0-5d66-4864-8668-3f55efe43ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df8da5-e09a-4bfb-96fb-c2e280ad9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./worddata/data/ratings_train.txt','r',encoding= 'utf-8')\n",
    "rdr = csv.reader(f, delimiter='\\t')\n",
    "rdw = list(rdr)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9fcbec-7e85-441b-943d-ed781d78cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a6f4aa-d5fc-49ac-bdc2-7e8a8cbcdb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [ ]\n",
    "for line in rdw:\n",
    "    malist = twitter.pos(line[1],norm= True, stem = True)\n",
    "    r = []\n",
    "    for word in malist:\n",
    "        if not wor[1] in ['josa','Eomi','Punctuation']\n",
    "        r.append(word[0])\n",
    "    r | = (''.join(r)).strip()\n",
    "    result.append(r|)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c35557-6db4-4ddb-810d-f88933f26162",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Okt()\n",
    "result = []\n",
    "for line in rdw:\n",
    "    malist = twitter.pos( line[1], norm=True, stem=True)\n",
    "    r = []\n",
    "    for word in malist:\n",
    "        if not word[1] in [\"Josa\",\"Eomi\",\"Punctuation\"]:\n",
    "            r.append(word[0])\n",
    "    rl = (\" \".join(r)).strip()\n",
    "    result.append(rl)\n",
    "    print(rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a516333-6a4a-4cb5-b0c6-f3d6b6364583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff55cd-2c30-42a2-a85c-0b1efee91790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46469684-4a2a-4d2c-b5a5-8b30dc4d6768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "text_sample = 'Natural Language Processing, or NLP, is the process of extracting the meaning, or intent, behind human language. In the field of Conversational artificial intelligence (AI), NLP allows machines and applications to understand the intent of human language inputs, and then generate appropriate responses, resulting in a natural conversation flow.'\n",
    "tokenized_sentences = sent_tokenize(text_sample)\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5deecc4-3c96-4132-b56d-7256f732a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "text_sample = 'Natural Language Processinglows.'\n",
    "word = word_tokenize(text_sample)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3966ce0-b33a-4348-8a06-a4d97394943a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02003c-cea3-406e-897c-ad46cf00a744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b83df1-7988-4902-8d8b-4d564dae43a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e646959-908c-4126-af30-b31c2b859522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it’s', 'nothing', 'that', 'you', 'don’t', 'already', 'know', 'except', 'most', 'people', 'aren’t', 'aware', 'of', 'how', 'their', 'inner', 'world', 'works']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(words)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaverMovie.nlp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m----> 6\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mresult\u001b[49m))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "sentence = \"it’s nothing that you don’t already know except most people aren’t aware of how their inner world works.\"\n",
    "words = text_to_word_sequence(sentence)\n",
    "print(words)\n",
    "with open(\"NaverMovie.nlp\",'w', encoding='utf-8') as fp:\n",
    "    fp.write(\"\\n\".join(result))\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "sample_text = \"One of the first things that we ask ourselves is what are the pros and cons of any task we perform.\"\n",
    "text_tokens = word_tokenize(sample_text)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
    "print(\"불용어 제거 미적용:\", text_tokens, '\\n')\n",
    "print(\"불용어 제거 적용:\",tokens_without_sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c5490a-6f1b-4c4a-a6dd-dbea3b3d61c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obess obsses\n",
      "standard standard\n",
      "nation nation\n",
      "absent absent\n",
      "tribal tribalic\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#포터 알고리즘\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('obesses'),stemmer.stem('obssesed'))\n",
    "print(stemmer.stem('standardizes'),stemmer.stem('standardization'))\n",
    "print(stemmer.stem('national'), stemmer.stem('nation'))\n",
    "print(stemmer.stem('absentness'), stemmer.stem('absently'))\n",
    "print(stemmer.stem('tribalical'), stemmer.stem('tribalicalized'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9bbf08-67c8-41e5-b775-0e79e8621355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# . 자기자신 .. 그 위에 것 생략하면 자기 자신 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137a6be-6bae-401a-8c49-93bb09502709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ab373cf-1ac2-4a76-93f2-4a54266bb7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obsess obsess\n",
      "standardizes standardization\n",
      "national nation\n",
      "absentness absently\n",
      "tribalical tribalicalized\n"
     ]
    }
   ],
   "source": [
    "#표제어 추출(Lemmatization)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "print(stemmer.stem('obsesses'),stemmer.stem('obsessed'))\n",
    "print(lemma.lemmatize('standardizes'),lemma.lemmatize('standardization'))\n",
    "print(lemma.lemmatize('national'), lemma.lemmatize('nation'))\n",
    "print(lemma.lemmatize('absentness'), lemma.lemmatize('absently'))\n",
    "print(lemma.lemmatize('tribalical'), lemma.lemmatize('tribalicalized'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb605550-4ac2-43b1-991e-4ae8ef2e9da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fcf9d3-acc7-4fd0-b5d1-da850a68ea38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49631536-55fc-4e11-8012-ad1808b6742c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f1da1-a975-4c52-9e65-a4546694601e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb4671-f13b-4049-9158-2969fa29574b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd7ead7-b07a-4892-9893-67b63c140ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae01d87-11c6-43f8-a3a9-a2809f82f49f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
