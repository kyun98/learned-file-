{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8527829f",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba481e95",
   "metadata": {},
   "source": [
    "## Ïù¥Î°†Ï†Å Í∞úÏöî"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d50dac2",
   "metadata": {},
   "source": [
    "Îã§ÏùåÍ≥º Í∞ôÏùÄ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÏäµÎãàÎã§.     \n",
    "ÏïûÏÑú Î∞∞Ïö¥ Îã®Ïàú ÏÑ†Ìòï ÌöåÍ∑ÄÏôÄ Îã§Î•∏ Ï†êÏùÄ ÎèÖÎ¶Ω Î≥ÄÏàò Ïùò Í∞úÏàòÍ∞Ä Ïù¥Ï†ú 1Í∞úÍ∞Ä ÏïÑÎãå 3Í∞úÎùºÎäî Ï†êÏûÖÎãàÎã§.     \n",
    "3Í∞úÏùò ÌÄ¥Ï¶à Ï†êÏàòÎ°úÎ∂ÄÌÑ∞ ÏµúÏ¢Ö Ï†êÏàòÎ•º ÏòàÏ∏°ÌïòÎäî Î™®Îç∏ÏùÑ ÎßåÎì§Ïñ¥Î≥¥Í≤†ÏäµÎãàÎã§.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bde54d",
   "metadata": {},
   "source": [
    "<img src=./LinearRegression_01.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478ac8d",
   "metadata": {},
   "source": [
    "ÎèÖÎ¶Ω Î≥ÄÏàò$x$ Ïùò Í∞úÏàòÍ∞Ä 3Í∞úÎØÄÎ°ú Ïù¥Î•º ÏàòÏãùÏúºÎ°ú ÌëúÌòÑÌïòÎ©¥ ÏïÑÎûòÏôÄ Í∞ôÏäµÎãàÎã§\n",
    "\n",
    "$ H(x) = x_1w_1 + x_2w_2 + x_3w_3 + b $   \n",
    "\n",
    "$ cost(W, b) = \\frac{1}{m} \\sum^m_{i=1} \\left( H(x^{(i)}) - y^{(i)} \\right)^2 $   \n",
    "\n",
    " - $H(x)$: Ï£ºÏñ¥ÏßÑ $x$ Í∞íÏóê ÎåÄÌï¥ ÏòàÏ∏°ÏùÑ Ïñ¥ÎñªÍ≤å Ìï† Í≤ÉÏù∏Í∞Ä ?\n",
    " - $cost(W, b)$: $H(x)$ Í∞Ä $y$ Î•º ÏñºÎßàÎÇò Ïûò ÏòàÏ∏°ÌñàÎäîÍ∞Ä ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5dbbf2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0352ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a6069f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x200ee7b2b70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85c18b7",
   "metadata": {},
   "source": [
    "## 1.3  Naive Data Representation\n",
    "\n",
    "$ H(x_1, x_2, x_3) = x_1w_1 + x_2w_2 + x_3w_3 + b $ \n",
    "\n",
    "ÏúÑÏùò ÏãùÏùÑ Î≥¥Î©¥ Ïù¥Î≤àÏóêÎäî Îã®Ïàú ÏÑ†Ìòï ÌöåÍ∑ÄÏôÄ Îã§Î•¥Í≤å  ùë•  Ïùò Í∞úÏàòÍ∞Ä 3Í∞úÏûÖÎãàÎã§. Í∑∏Îü¨ÎãàÍπå Î•º 3Í∞ú ÏÑ†Ïñ∏Ìï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4adef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞\n",
    "x1_train = torch.FloatTensor([[73],\n",
    "                              [93],\n",
    "                              [89],\n",
    "                              [96],\n",
    "                              [73]])\n",
    "x2_train = torch.FloatTensor([[80],\n",
    "                              [88],\n",
    "                              [91],\n",
    "                              [98],\n",
    "                              [66]])\n",
    "x3_train = torch.FloatTensor([[75],\n",
    "                              [93],\n",
    "                              [90],\n",
    "                              [100],\n",
    "                              [70]])\n",
    "y_train = torch.FloatTensor([[152],\n",
    "                             [185],\n",
    "                             [180],\n",
    "                             [196],\n",
    "                             [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d9771",
   "metadata": {},
   "source": [
    "Ïù¥Ï†ú Í∞ÄÏ§ëÏπò $W$ ÏôÄ Ìé∏Ìñ• $b$ Î•º ÏÑ†Ïñ∏Ìï©ÎãàÎã§. Í∞ÄÏ§ëÏπò $W$ ÎèÑ 3Í∞ú ÏÑ†Ïñ∏Ìï¥Ï£ºÏñ¥Ïïº Ìï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "267729bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Í∞ÄÏ§ëÏπò wÏôÄ Ìé∏Ìñ• b Ï¥àÍ∏∞Ìôî\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4fce492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 : 0.0\n",
      "w2 : 0.0\n",
      "w2 : 0.0\n",
      "b  : 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"w1 :\", w1.item())\n",
    "print(\"w2 :\", w2.item())\n",
    "print(\"w2 :\", w2.item())\n",
    "print(\"b  :\", b.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "656e3bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
      "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563634\n",
      "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497607\n",
      "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435026\n",
      "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375730\n",
      "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319511\n",
      "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n",
      "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215696\n",
      "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167818\n",
      "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079378\n"
     ]
    }
   ],
   "source": [
    "# optimizer ÏÑ§Ï†ï\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) Í≥ÑÏÇ∞\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "\n",
    "    # cost Í≥ÑÏÇ∞\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # costÎ°ú H(x) Í∞úÏÑ†\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100Î≤àÎßàÎã§ Î°úÍ∑∏ Ï∂úÎ†•\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e11e0",
   "metadata": {},
   "source": [
    "ÏúÑÏùò Í≤ΩÏö∞ Í∞ÄÏÑ§ÏùÑ ÏÑ†Ïñ∏ÌïòÎäî Î∂ÄÎ∂ÑÏù∏ hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + bÏóêÏÑúÎèÑ     \n",
    "x_trainÏùò Í∞úÏàòÎßåÌÅº wÏôÄ Í≥±Ìï¥Ï£ºÎèÑÎ°ù ÏûëÏÑ±Ìï¥Ï§Ä Í≤ÉÏùÑ ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "810a9a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152.454\n"
     ]
    }
   ],
   "source": [
    "pred = (73*0.718 +  80* 0.613 +  75 * 0.680) \n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd87cdb",
   "metadata": {},
   "source": [
    "$H(X) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3}$\n",
    "\n",
    "ÏúÑ ÏãùÏùÄ ÏïÑÎûòÏôÄ Í∞ôÏù¥ Îëê Î≤°ÌÑ∞Ïùò ÎÇ¥Ï†ÅÏúºÎ°ú ÌëúÌòÑÌï† Ïàò ÏûàÏäµÎãàÎã§."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734e8a1",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{pmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "w_3 \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "x_1w_1 + x_2w_2 + x_3w_3\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb5a75",
   "metadata": {},
   "source": [
    "Îëê Î≤°ÌÑ∞Î•º Í∞ÅÍ∞Å $X$ÏôÄ $W$Î°ú ÌëúÌòÑÌïúÎã§Î©¥, Í∞ÄÏÑ§ÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§.\n",
    "\n",
    "$ H(X) = XW $\n",
    "\n",
    "$x$Ïùò Í∞úÏàòÍ∞Ä 3Í∞úÏòÄÏùåÏóêÎèÑ Ïù¥Ï†úÎäî $X$ÏôÄ $W$ÎùºÎäî Îëê Í∞úÏùò Î≥ÄÏàòÎ°ú ÌëúÌòÑÎêú Í≤ÉÏùÑ Î≥º Ïàò ÏûàÏäµÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb32359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152],\n",
    "                             [185],\n",
    "                             [180],\n",
    "                             [196],\n",
    "                             [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ade3b",
   "metadata": {},
   "source": [
    "Ïù¥Îäî ÎèÖÎ¶Ω Î≥ÄÏàò $x$ Îì§Ïùò ÏàòÍ∞Ä (ÏÉòÌîåÏùò Ïàò √ó ÌäπÏÑ±Ïùò Ïàò) = 15Í∞úÏûÑÏùÑ ÏùòÎØ∏Ìï©ÎãàÎã§. ÎèÖÎ¶Ω Î≥ÄÏàò $x$Îì§ÏùÑ (ÏÉòÌîåÏùò Ïàò √ó ÌäπÏÑ±Ïùò Ïàò)Ïùò ÌÅ¨Í∏∞Î•º Í∞ÄÏßÄÎäî ÌïòÎÇòÏùò ÌñâÎ†¨Î°ú ÌëúÌòÑÌï¥Î¥ÖÏãúÎã§. Í∑∏Î¶¨Í≥† Ïù¥ ÌñâÎ†¨ÏùÑ $X$ÎùºÍ≥† ÌïòÍ≤†ÏäµÎãàÎã§.\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "    \\begin{array}{c}\n",
    "      x_{11}\\ x_{12}\\ x_{13}\\ \\\\\n",
    "      x_{21}\\ x_{22}\\ x_{23}\\ \\\\\n",
    "      x_{31}\\ x_{32}\\ x_{33}\\ \\\\\n",
    "      x_{41}\\ x_{42}\\ x_{43}\\ \\\\\n",
    "      x_{51}\\ x_{52}\\ x_{53}\\ \\\\\n",
    "    \\end{array}\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67259c91",
   "metadata": {},
   "source": [
    "Í∑∏Î¶¨Í≥† Ïó¨Í∏∞Ïóê Í∞ÄÏ§ëÏπò $w1, w2, w3$ ÏùÑ ÏõêÏÜåÎ°ú ÌïòÎäî Î≤°ÌÑ∞Î•º $X$ Îùº ÌïòÍ≥† Ïù¥Î•º Í≥±Ìï¥Î≥¥Í≤†ÏäµÎãàÎã§.\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "   \\begin{array}{c}\n",
    "     x_{11}\\ x_{12}\\ x_{13}\\ \\\\\n",
    "     x_{21}\\ x_{22}\\ x_{23}\\ \\\\\n",
    "     x_{31}\\ x_{32}\\ x_{33}\\ \\\\\n",
    "     x_{41}\\ x_{42}\\ x_{43}\\ \\\\\n",
    "     x_{51}\\ x_{52}\\ x_{53}\\ \\\\\n",
    "   \\end{array}\n",
    "\\end{pmatrix}\n",
    "$\n",
    "$\\begin{pmatrix}\n",
    "    \\begin{array}{c}\n",
    "      w_{1} \\\\\n",
    "      w_{2} \\\\\n",
    "      w_{3} \\\\\n",
    "    \\end{array}\n",
    "\\end{pmatrix}$\n",
    "\\ =\n",
    "$\\begin{pmatrix}\n",
    "    \\begin{array}{c}\n",
    "      x_{11}w_{1}+ x_{12}w_{2}+ x_{13}w_{3}\\ \\\\\n",
    "      x_{21}w_{1}+ x_{22}w_{2}+ x_{23}w_{3}\\ \\\\\n",
    "      x_{31}w_{1}+ x_{32}w_{2}+ x_{33}w_{3}\\ \\\\\n",
    "      x_{41}w_{1}+ x_{42}w_{2}+ x_{43}w_{3}\\ \\\\\n",
    "      x_{51}w_{1}+ x_{52}w_{2}+ x_{53}w_{3}\\ \\\\\n",
    "    \\end{array}\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c270b",
   "metadata": {},
   "source": [
    "ÏúÑÏùò ÏãùÏùÄ Í≤∞Í≥ºÏ†ÅÏúºÎ°ú Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§.   \n",
    "   \n",
    "$H(X) = XW$\n",
    "\n",
    "Ïù¥ Í∞ÄÏÑ§Ïóê Í∞Å ÏÉòÌîåÏóê ÎçîÌï¥ÏßÄÎäî Ìé∏Ìñ• $b$ Î•º Ï∂îÍ∞ÄÌï¥Î¥ÖÏãúÎã§. ÏÉòÌîå ÏàòÎßåÌÅºÏùò Ï∞®ÏõêÏùÑ Í∞ÄÏßÄÎäî Ìé∏Ìñ• Î≤°ÌÑ∞$B$ Î•º ÎßåÎì§Ïñ¥ ÎçîÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b94de",
   "metadata": {},
   "source": [
    "$\\begin{pmatrix}\n",
    "    \\begin{array}{c}\n",
    "      x_{11}\\ x_{12}\\ x_{13}\\ \\\\\n",
    "      x_{21}\\ x_{22}\\ x_{23}\\ \\\\\n",
    "      x_{31}\\ x_{32}\\ x_{33}\\ \\\\\n",
    "      x_{41}\\ x_{42}\\ x_{43}\\ \\\\\n",
    "      x_{51}\\ x_{52}\\ x_{53}\\ \\\\\n",
    "    \\end{array}\n",
    "\\end{pmatrix}$\n",
    "$\\begin{pmatrix}\n",
    "    \\begin{array}{c}\n",
    "      w_{1} \\\\\n",
    "      w_{2} \\\\\n",
    "      w_{3} \\\\\n",
    "    \\end{array}\n",
    "\\end{pmatrix}$\n",
    "$+$\n",
    "$\\begin{pmatrix}\n",
    "    \\begin{array}{c}\n",
    "      b \\\\\n",
    "      b \\\\\n",
    "      b \\\\\n",
    "      b \\\\\n",
    "      b \\\\\n",
    "    \\end{array}\n",
    "\\end{pmatrix}$\n",
    "$\\ =$\n",
    "$\\begin{pmatrix}\n",
    "    \\begin{array}{c}\n",
    "      x_{11}w_{1}+ x_{12}w_{2}+ x_{13}w_{3} + b\\ \\\\\n",
    "      x_{21}w_{1}+ x_{22}w_{2}+ x_{23}w_{3} + b\\ \\\\\n",
    "      x_{31}w_{1}+ x_{32}w_{2}+ x_{33}w_{3} + b\\ \\\\\n",
    "      x_{41}w_{1}+ x_{42}w_{2}+ x_{43}w_{3} + b\\ \\\\\n",
    "      x_{51}w_{1}+ x_{52}w_{2}+ x_{53}w_{3} + b\\ \\\\\n",
    "    \\end{array}\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc26f84c",
   "metadata": {},
   "source": [
    "ÏúÑÏùò ÏãùÏùÄ Í≤∞Í≥ºÏ†ÅÏúºÎ°ú Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§.\n",
    "\n",
    "$H(X) = XW + B$\n",
    "\n",
    "Í≤∞Í≥ºÏ†ÅÏúºÎ°ú Ï†ÑÏ≤¥ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïùò Í∞ÄÏÑ§ Ïó∞ÏÇ∞ÏùÑ 3Í∞úÏùò Î≥ÄÏàòÎßåÏúºÎ°ú ÌëúÌòÑÌïòÏòÄÏäµÎãàÎã§.   \n",
    "Ïù¥ÏôÄ Í∞ôÏù¥ Î≤°ÌÑ∞ÏôÄ ÌñâÎ†¨ Ïó∞ÏÇ∞ÏùÄ ÏãùÏùÑ Í∞ÑÎã®ÌïòÍ≤å Ìï¥Ï§Ñ ÎøêÎßå ÏïÑÎãàÎùº Îã§ÏàòÏùò ÏÉòÌîåÏùò Î≥ëÎ†¨ Ïó∞ÏÇ∞Ïù¥ÎØÄÎ°ú ÏÜçÎèÑÏùò Ïù¥Ï†êÏùÑ Í∞ÄÏßëÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf9d2742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdd1f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Í∞ÄÏ§ëÏπò wÏôÄ Ìé∏Ìñ• b Ï¥àÍ∏∞Ìôî\n",
    "W = torch.zeros((3,1), requires_grad=True)\n",
    "\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f7c3677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/30 hypothesis: tensor([152.7939, 183.6813, 180.9665, 197.0692, 140.1102])  Cost: 1.603586\n",
      "Epoch    1/30 hypothesis: tensor([152.7935, 183.6816, 180.9664, 197.0691, 140.1106])  Cost: 1.602878\n",
      "Epoch    2/30 hypothesis: tensor([152.7931, 183.6819, 180.9663, 197.0690, 140.1110])  Cost: 1.602179\n",
      "Epoch    3/30 hypothesis: tensor([152.7926, 183.6822, 180.9661, 197.0689, 140.1114])  Cost: 1.601486\n",
      "Epoch    4/30 hypothesis: tensor([152.7922, 183.6826, 180.9660, 197.0688, 140.1118])  Cost: 1.600765\n",
      "Epoch    5/30 hypothesis: tensor([152.7918, 183.6828, 180.9659, 197.0686, 140.1122])  Cost: 1.600072\n",
      "Epoch    6/30 hypothesis: tensor([152.7914, 183.6831, 180.9657, 197.0685, 140.1126])  Cost: 1.599369\n",
      "Epoch    7/30 hypothesis: tensor([152.7909, 183.6834, 180.9656, 197.0684, 140.1130])  Cost: 1.598692\n",
      "Epoch    8/30 hypothesis: tensor([152.7905, 183.6837, 180.9655, 197.0683, 140.1134])  Cost: 1.597991\n",
      "Epoch    9/30 hypothesis: tensor([152.7900, 183.6840, 180.9653, 197.0682, 140.1138])  Cost: 1.597289\n",
      "Epoch   10/30 hypothesis: tensor([152.7896, 183.6843, 180.9652, 197.0681, 140.1143])  Cost: 1.596590\n",
      "Epoch   11/30 hypothesis: tensor([152.7892, 183.6846, 180.9651, 197.0679, 140.1147])  Cost: 1.595898\n",
      "Epoch   12/30 hypothesis: tensor([152.7888, 183.6849, 180.9650, 197.0678, 140.1151])  Cost: 1.595198\n",
      "Epoch   13/30 hypothesis: tensor([152.7883, 183.6852, 180.9648, 197.0677, 140.1155])  Cost: 1.594514\n",
      "Epoch   14/30 hypothesis: tensor([152.7879, 183.6855, 180.9647, 197.0676, 140.1159])  Cost: 1.593821\n",
      "Epoch   15/30 hypothesis: tensor([152.7875, 183.6858, 180.9646, 197.0675, 140.1163])  Cost: 1.593129\n",
      "Epoch   16/30 hypothesis: tensor([152.7870, 183.6861, 180.9644, 197.0674, 140.1167])  Cost: 1.592433\n",
      "Epoch   17/30 hypothesis: tensor([152.7866, 183.6864, 180.9643, 197.0673, 140.1171])  Cost: 1.591737\n",
      "Epoch   18/30 hypothesis: tensor([152.7862, 183.6867, 180.9642, 197.0672, 140.1175])  Cost: 1.591038\n",
      "Epoch   19/30 hypothesis: tensor([152.7858, 183.6870, 180.9641, 197.0670, 140.1179])  Cost: 1.590336\n",
      "Epoch   20/30 hypothesis: tensor([152.7853, 183.6873, 180.9639, 197.0669, 140.1183])  Cost: 1.589665\n",
      "Epoch   21/30 hypothesis: tensor([152.7849, 183.6876, 180.9638, 197.0668, 140.1187])  Cost: 1.588963\n",
      "Epoch   22/30 hypothesis: tensor([152.7845, 183.6879, 180.9637, 197.0667, 140.1191])  Cost: 1.588273\n",
      "Epoch   23/30 hypothesis: tensor([152.7840, 183.6882, 180.9635, 197.0666, 140.1195])  Cost: 1.587576\n",
      "Epoch   24/30 hypothesis: tensor([152.7836, 183.6885, 180.9634, 197.0665, 140.1199])  Cost: 1.586890\n",
      "Epoch   25/30 hypothesis: tensor([152.7832, 183.6888, 180.9633, 197.0664, 140.1203])  Cost: 1.586196\n",
      "Epoch   26/30 hypothesis: tensor([152.7828, 183.6890, 180.9632, 197.0663, 140.1207])  Cost: 1.585522\n",
      "Epoch   27/30 hypothesis: tensor([152.7823, 183.6893, 180.9630, 197.0661, 140.1211])  Cost: 1.584821\n",
      "Epoch   28/30 hypothesis: tensor([152.7819, 183.6896, 180.9629, 197.0660, 140.1215])  Cost: 1.584131\n",
      "Epoch   29/30 hypothesis: tensor([152.7815, 183.6899, 180.9628, 197.0659, 140.1219])  Cost: 1.583444\n",
      "Epoch   30/30 hypothesis: tensor([152.7810, 183.6902, 180.9626, 197.0658, 140.1223])  Cost: 1.582750\n"
     ]
    }
   ],
   "source": [
    "# optimizer ÏÑ§Ï†ï\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 30\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) Í≥ÑÏÇ∞\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "\n",
    "    # cost Í≥ÑÏÇ∞\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # costÎ°ú H(x) Í∞úÏÑ†\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} hypothesis: {}  Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b58811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1],\n",
    "                             [2],\n",
    "                             [3],\n",
    "                             [4],\n",
    "                             [5]])\n",
    "y_train = torch.FloatTensor([[2],\n",
    "                             [4],\n",
    "                             [6],\n",
    "                             [8],\n",
    "                             [10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e751a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8922732e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 hypothesis: tensor([-0.0245,  0.3408,  0.7060,  1.0712,  1.4364])  Cost: 33.371571\n",
      "Epoch    1/100 hypothesis: tensor([0.4644, 1.2127, 1.9610, 2.7092, 3.4575])  Cost: 19.447556\n",
      "Epoch    2/100 hypothesis: tensor([0.8376, 1.8783, 2.9190, 3.9596, 5.0003])  Cost: 11.333327\n",
      "Epoch    3/100 hypothesis: tensor([1.1225, 2.3864, 3.6503, 4.9142, 6.1781])  Cost: 6.604755\n",
      "Epoch    4/100 hypothesis: tensor([1.3399, 2.7742, 4.2086, 5.6429, 7.0772])  Cost: 3.849176\n",
      "Epoch    5/100 hypothesis: tensor([1.5058, 3.0703, 4.6347, 6.1992, 7.7636])  Cost: 2.243360\n",
      "Epoch    6/100 hypothesis: tensor([1.6325, 3.2963, 4.9601, 6.6238, 8.2876])  Cost: 1.307568\n",
      "Epoch    7/100 hypothesis: tensor([1.7291, 3.4688, 5.2084, 6.9480, 8.6876])  Cost: 0.762231\n",
      "Epoch    8/100 hypothesis: tensor([1.8029, 3.6004, 5.3980, 7.1955, 8.9930])  Cost: 0.444436\n",
      "Epoch    9/100 hypothesis: tensor([1.8591, 3.7009, 5.5427, 7.3844, 9.2262])  Cost: 0.259238\n",
      "Epoch   10/100 hypothesis: tensor([1.9021, 3.7776, 5.6531, 7.5286, 9.4042])  Cost: 0.151313\n",
      "Epoch   11/100 hypothesis: tensor([1.9348, 3.8361, 5.7374, 7.6388, 9.5401])  Cost: 0.088417\n",
      "Epoch   12/100 hypothesis: tensor([1.9597, 3.8808, 5.8018, 7.7228, 9.6438])  Cost: 0.051763\n",
      "Epoch   13/100 hypothesis: tensor([1.9788, 3.9148, 5.8509, 7.7870, 9.7231])  Cost: 0.030402\n",
      "Epoch   14/100 hypothesis: tensor([1.9932, 3.9408, 5.8884, 7.8360, 9.7836])  Cost: 0.017952\n",
      "Epoch   15/100 hypothesis: tensor([2.0043, 3.9606, 5.9170, 7.8734, 9.8297])  Cost: 0.010695\n",
      "Epoch   16/100 hypothesis: tensor([2.0126, 3.9757, 5.9388, 7.9019, 9.8650])  Cost: 0.006464\n",
      "Epoch   17/100 hypothesis: tensor([2.0190, 3.9873, 5.9555, 7.9237, 9.8920])  Cost: 0.003997\n",
      "Epoch   18/100 hypothesis: tensor([2.0238, 3.9960, 5.9682, 7.9404, 9.9126])  Cost: 0.002558\n",
      "Epoch   19/100 hypothesis: tensor([2.0275, 4.0027, 5.9779, 7.9531, 9.9283])  Cost: 0.001718\n",
      "Epoch   20/100 hypothesis: tensor([2.0303, 4.0078, 5.9853, 7.9628, 9.9403])  Cost: 0.001227\n",
      "Epoch   21/100 hypothesis: tensor([2.0323, 4.0116, 5.9909, 7.9702, 9.9495])  Cost: 0.000939\n",
      "Epoch   22/100 hypothesis: tensor([2.0339, 4.0146, 5.9952, 7.9759, 9.9566])  Cost: 0.000770\n",
      "Epoch   23/100 hypothesis: tensor([2.0350, 4.0168, 5.9985, 7.9802, 9.9620])  Cost: 0.000670\n",
      "Epoch   24/100 hypothesis: tensor([2.0359, 4.0184, 6.0010, 7.9835, 9.9661])  Cost: 0.000610\n",
      "Epoch   25/100 hypothesis: tensor([2.0365, 4.0197, 6.0029, 7.9861, 9.9693])  Cost: 0.000574\n",
      "Epoch   26/100 hypothesis: tensor([2.0370, 4.0206, 6.0043, 7.9880, 9.9717])  Cost: 0.000551\n",
      "Epoch   27/100 hypothesis: tensor([2.0373, 4.0213, 6.0054, 7.9895, 9.9736])  Cost: 0.000536\n",
      "Epoch   28/100 hypothesis: tensor([2.0375, 4.0219, 6.0062, 7.9906, 9.9750])  Cost: 0.000526\n",
      "Epoch   29/100 hypothesis: tensor([2.0376, 4.0222, 6.0069, 7.9915, 9.9762])  Cost: 0.000519\n",
      "Epoch   30/100 hypothesis: tensor([2.0377, 4.0225, 6.0073, 7.9922, 9.9770])  Cost: 0.000513\n",
      "Epoch   31/100 hypothesis: tensor([2.0377, 4.0227, 6.0077, 7.9927, 9.9777])  Cost: 0.000509\n",
      "Epoch   32/100 hypothesis: tensor([2.0377, 4.0228, 6.0080, 7.9931, 9.9782])  Cost: 0.000504\n",
      "Epoch   33/100 hypothesis: tensor([2.0376, 4.0229, 6.0081, 7.9934, 9.9787])  Cost: 0.000501\n",
      "Epoch   34/100 hypothesis: tensor([2.0376, 4.0229, 6.0083, 7.9936, 9.9790])  Cost: 0.000497\n",
      "Epoch   35/100 hypothesis: tensor([2.0375, 4.0229, 6.0084, 7.9938, 9.9793])  Cost: 0.000493\n",
      "Epoch   36/100 hypothesis: tensor([2.0374, 4.0229, 6.0085, 7.9940, 9.9795])  Cost: 0.000490\n",
      "Epoch   37/100 hypothesis: tensor([2.0373, 4.0229, 6.0085, 7.9941, 9.9797])  Cost: 0.000487\n",
      "Epoch   38/100 hypothesis: tensor([2.0372, 4.0229, 6.0085, 7.9942, 9.9799])  Cost: 0.000483\n",
      "Epoch   39/100 hypothesis: tensor([2.0371, 4.0228, 6.0085, 7.9943, 9.9800])  Cost: 0.000480\n",
      "Epoch   40/100 hypothesis: tensor([2.0370, 4.0228, 6.0085, 7.9943, 9.9801])  Cost: 0.000477\n",
      "Epoch   41/100 hypothesis: tensor([2.0368, 4.0227, 6.0085, 7.9944, 9.9802])  Cost: 0.000474\n",
      "Epoch   42/100 hypothesis: tensor([2.0367, 4.0226, 6.0085, 7.9944, 9.9803])  Cost: 0.000470\n",
      "Epoch   43/100 hypothesis: tensor([2.0366, 4.0226, 6.0085, 7.9945, 9.9804])  Cost: 0.000467\n",
      "Epoch   44/100 hypothesis: tensor([2.0365, 4.0225, 6.0085, 7.9945, 9.9805])  Cost: 0.000464\n",
      "Epoch   45/100 hypothesis: tensor([2.0364, 4.0224, 6.0085, 7.9945, 9.9806])  Cost: 0.000461\n",
      "Epoch   46/100 hypothesis: tensor([2.0363, 4.0224, 6.0085, 7.9946, 9.9807])  Cost: 0.000458\n",
      "Epoch   47/100 hypothesis: tensor([2.0361, 4.0223, 6.0084, 7.9946, 9.9807])  Cost: 0.000455\n",
      "Epoch   48/100 hypothesis: tensor([2.0360, 4.0222, 6.0084, 7.9946, 9.9808])  Cost: 0.000452\n",
      "Epoch   49/100 hypothesis: tensor([2.0359, 4.0221, 6.0084, 7.9946, 9.9809])  Cost: 0.000449\n",
      "Epoch   50/100 hypothesis: tensor([2.0358, 4.0221, 6.0084, 7.9947, 9.9809])  Cost: 0.000446\n",
      "Epoch   51/100 hypothesis: tensor([2.0357, 4.0220, 6.0083, 7.9947, 9.9810])  Cost: 0.000443\n",
      "Epoch   52/100 hypothesis: tensor([2.0355, 4.0219, 6.0083, 7.9947, 9.9811])  Cost: 0.000440\n",
      "Epoch   53/100 hypothesis: tensor([2.0354, 4.0218, 6.0083, 7.9947, 9.9811])  Cost: 0.000437\n",
      "Epoch   54/100 hypothesis: tensor([2.0353, 4.0218, 6.0082, 7.9947, 9.9812])  Cost: 0.000434\n",
      "Epoch   55/100 hypothesis: tensor([2.0352, 4.0217, 6.0082, 7.9947, 9.9813])  Cost: 0.000431\n",
      "Epoch   56/100 hypothesis: tensor([2.0351, 4.0216, 6.0082, 7.9948, 9.9813])  Cost: 0.000428\n",
      "Epoch   57/100 hypothesis: tensor([2.0349, 4.0216, 6.0082, 7.9948, 9.9814])  Cost: 0.000425\n",
      "Epoch   58/100 hypothesis: tensor([2.0348, 4.0215, 6.0081, 7.9948, 9.9815])  Cost: 0.000422\n",
      "Epoch   59/100 hypothesis: tensor([2.0347, 4.0214, 6.0081, 7.9948, 9.9815])  Cost: 0.000419\n",
      "Epoch   60/100 hypothesis: tensor([2.0346, 4.0213, 6.0081, 7.9948, 9.9816])  Cost: 0.000416\n",
      "Epoch   61/100 hypothesis: tensor([2.0345, 4.0213, 6.0081, 7.9949, 9.9817])  Cost: 0.000414\n",
      "Epoch   62/100 hypothesis: tensor([2.0343, 4.0212, 6.0080, 7.9949, 9.9817])  Cost: 0.000411\n",
      "Epoch   63/100 hypothesis: tensor([2.0342, 4.0211, 6.0080, 7.9949, 9.9818])  Cost: 0.000408\n",
      "Epoch   64/100 hypothesis: tensor([2.0341, 4.0210, 6.0080, 7.9949, 9.9818])  Cost: 0.000405\n",
      "Epoch   65/100 hypothesis: tensor([2.0340, 4.0210, 6.0079, 7.9949, 9.9819])  Cost: 0.000403\n",
      "Epoch   66/100 hypothesis: tensor([2.0339, 4.0209, 6.0079, 7.9949, 9.9820])  Cost: 0.000400\n",
      "Epoch   67/100 hypothesis: tensor([2.0338, 4.0208, 6.0079, 7.9950, 9.9820])  Cost: 0.000397\n",
      "Epoch   68/100 hypothesis: tensor([2.0337, 4.0208, 6.0079, 7.9950, 9.9821])  Cost: 0.000394\n",
      "Epoch   69/100 hypothesis: tensor([2.0335, 4.0207, 6.0078, 7.9950, 9.9821])  Cost: 0.000392\n",
      "Epoch   70/100 hypothesis: tensor([2.0334, 4.0206, 6.0078, 7.9950, 9.9822])  Cost: 0.000389\n",
      "Epoch   71/100 hypothesis: tensor([2.0333, 4.0206, 6.0078, 7.9950, 9.9823])  Cost: 0.000387\n",
      "Epoch   72/100 hypothesis: tensor([2.0332, 4.0205, 6.0078, 7.9950, 9.9823])  Cost: 0.000384\n",
      "Epoch   73/100 hypothesis: tensor([2.0331, 4.0204, 6.0077, 7.9951, 9.9824])  Cost: 0.000381\n",
      "Epoch   74/100 hypothesis: tensor([2.0330, 4.0203, 6.0077, 7.9951, 9.9824])  Cost: 0.000379\n",
      "Epoch   75/100 hypothesis: tensor([2.0329, 4.0203, 6.0077, 7.9951, 9.9825])  Cost: 0.000376\n",
      "Epoch   76/100 hypothesis: tensor([2.0328, 4.0202, 6.0077, 7.9951, 9.9826])  Cost: 0.000374\n",
      "Epoch   77/100 hypothesis: tensor([2.0326, 4.0201, 6.0076, 7.9951, 9.9826])  Cost: 0.000371\n",
      "Epoch   78/100 hypothesis: tensor([2.0325, 4.0201, 6.0076, 7.9951, 9.9827])  Cost: 0.000369\n",
      "Epoch   79/100 hypothesis: tensor([2.0324, 4.0200, 6.0076, 7.9952, 9.9827])  Cost: 0.000366\n",
      "Epoch   80/100 hypothesis: tensor([2.0323, 4.0199, 6.0076, 7.9952, 9.9828])  Cost: 0.000364\n",
      "Epoch   81/100 hypothesis: tensor([2.0322, 4.0199, 6.0075, 7.9952, 9.9829])  Cost: 0.000361\n",
      "Epoch   82/100 hypothesis: tensor([2.0321, 4.0198, 6.0075, 7.9952, 9.9829])  Cost: 0.000359\n",
      "Epoch   83/100 hypothesis: tensor([2.0320, 4.0197, 6.0075, 7.9952, 9.9830])  Cost: 0.000356\n",
      "Epoch   84/100 hypothesis: tensor([2.0319, 4.0197, 6.0075, 7.9952, 9.9830])  Cost: 0.000354\n",
      "Epoch   85/100 hypothesis: tensor([2.0318, 4.0196, 6.0074, 7.9953, 9.9831])  Cost: 0.000352\n",
      "Epoch   86/100 hypothesis: tensor([2.0317, 4.0195, 6.0074, 7.9953, 9.9831])  Cost: 0.000349\n",
      "Epoch   87/100 hypothesis: tensor([2.0316, 4.0195, 6.0074, 7.9953, 9.9832])  Cost: 0.000347\n",
      "Epoch   88/100 hypothesis: tensor([2.0315, 4.0194, 6.0074, 7.9953, 9.9833])  Cost: 0.000344\n",
      "Epoch   89/100 hypothesis: tensor([2.0313, 4.0193, 6.0073, 7.9953, 9.9833])  Cost: 0.000342\n",
      "Epoch   90/100 hypothesis: tensor([2.0312, 4.0193, 6.0073, 7.9953, 9.9834])  Cost: 0.000340\n",
      "Epoch   91/100 hypothesis: tensor([2.0311, 4.0192, 6.0073, 7.9954, 9.9834])  Cost: 0.000338\n",
      "Epoch   92/100 hypothesis: tensor([2.0310, 4.0191, 6.0073, 7.9954, 9.9835])  Cost: 0.000335\n",
      "Epoch   93/100 hypothesis: tensor([2.0309, 4.0191, 6.0072, 7.9954, 9.9835])  Cost: 0.000333\n",
      "Epoch   94/100 hypothesis: tensor([2.0308, 4.0190, 6.0072, 7.9954, 9.9836])  Cost: 0.000331\n",
      "Epoch   95/100 hypothesis: tensor([2.0307, 4.0189, 6.0072, 7.9954, 9.9836])  Cost: 0.000329\n",
      "Epoch   96/100 hypothesis: tensor([2.0306, 4.0189, 6.0072, 7.9954, 9.9837])  Cost: 0.000326\n",
      "Epoch   97/100 hypothesis: tensor([2.0305, 4.0188, 6.0071, 7.9954, 9.9838])  Cost: 0.000324\n",
      "Epoch   98/100 hypothesis: tensor([2.0304, 4.0188, 6.0071, 7.9955, 9.9838])  Cost: 0.000322\n",
      "Epoch   99/100 hypothesis: tensor([2.0303, 4.0187, 6.0071, 7.9955, 9.9839])  Cost: 0.000320\n",
      "Epoch  100/100 hypothesis: tensor([2.0302, 4.0186, 6.0071, 7.9955, 9.9839])  Cost: 0.000318\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer ÏÑ§Ï†ï\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 100\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    # cost Í≥ÑÏÇ∞\n",
    "    cost = F.mse_loss(hypothesis , y_train)\n",
    "\n",
    "    # costÎ°ú H(x) Í∞úÏÑ†\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} hypothesis: {}  Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8815b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152],\n",
    "                             [185],\n",
    "                             [180],\n",
    "                             [196],\n",
    "                             [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68b7c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "019dca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([-6.7933, -4.8968, -6.5155, -7.3361, -2.6660])  Cost: 31667.599609\n",
      "Epoch    1/20 hypothesis: tensor([62.7036, 78.6330, 75.7880, 82.2903, 61.0462])  Cost: 9926.266602\n",
      "Epoch    2/20 hypothesis: tensor([101.6124, 125.3983, 121.8667, 132.4688,  96.7163])  Cost: 3111.513916\n",
      "Epoch    3/20 hypothesis: tensor([123.3960, 151.5805, 147.6645, 160.5620, 116.6867])  Cost: 975.451355\n",
      "Epoch    4/20 hypothesis: tensor([135.5919, 166.2389, 162.1078, 176.2903, 127.8673])  Cost: 305.908539\n",
      "Epoch    5/20 hypothesis: tensor([142.4200, 174.4456, 170.1940, 185.0960, 134.1269])  Cost: 96.042496\n",
      "Epoch    6/20 hypothesis: tensor([146.2428, 179.0401, 174.7213, 190.0260, 137.6314])  Cost: 30.260748\n",
      "Epoch    7/20 hypothesis: tensor([148.3830, 181.6125, 177.2559, 192.7861, 139.5934])  Cost: 9.641701\n",
      "Epoch    8/20 hypothesis: tensor([149.5814, 183.0526, 178.6749, 194.3314, 140.6919])  Cost: 3.178671\n",
      "Epoch    9/20 hypothesis: tensor([150.2523, 183.8588, 179.4694, 195.1966, 141.3068])  Cost: 1.152871\n",
      "Epoch   10/20 hypothesis: tensor([150.6279, 184.3102, 179.9142, 195.6810, 141.6511])  Cost: 0.517863\n",
      "Epoch   11/20 hypothesis: tensor([150.8383, 184.5629, 180.1633, 195.9521, 141.8438])  Cost: 0.318801\n",
      "Epoch   12/20 hypothesis: tensor([150.9561, 184.7043, 180.3027, 196.1040, 141.9516])  Cost: 0.256388\n",
      "Epoch   13/20 hypothesis: tensor([151.0221, 184.7835, 180.3808, 196.1890, 142.0120])  Cost: 0.236821\n",
      "Epoch   14/20 hypothesis: tensor([151.0591, 184.8278, 180.4245, 196.2366, 142.0458])  Cost: 0.230660\n",
      "Epoch   15/20 hypothesis: tensor([151.0798, 184.8526, 180.4490, 196.2632, 142.0646])  Cost: 0.228719\n",
      "Epoch   16/20 hypothesis: tensor([151.0914, 184.8664, 180.4627, 196.2782, 142.0752])  Cost: 0.228095\n",
      "Epoch   17/20 hypothesis: tensor([151.0980, 184.8741, 180.4704, 196.2865, 142.0811])  Cost: 0.227880\n",
      "Epoch   18/20 hypothesis: tensor([151.1017, 184.8784, 180.4747, 196.2912, 142.0843])  Cost: 0.227799\n",
      "Epoch   19/20 hypothesis: tensor([151.1038, 184.8808, 180.4772, 196.2939, 142.0861])  Cost: 0.227762\n",
      "Epoch   20/20 hypothesis: tensor([151.1050, 184.8821, 180.4786, 196.2953, 142.0871])  Cost: 0.227732\n"
     ]
    }
   ],
   "source": [
    "model = MultivariateLinearRegressionModel()\n",
    "\n",
    "# optimizer ÏÑ§Ï†ï\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    # cost Í≥ÑÏÇ∞\n",
    "    cost = F.mse_loss(hypothesis , y_train)\n",
    "\n",
    "    # costÎ°ú H(x) Í∞úÏÑ†\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} hypothesis: {}  Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c82601ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70f59c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data =  [[73, 80, 75],\n",
    "                        [93, 88, 93],\n",
    "                        [89, 91, 90],\n",
    "                        [96, 98, 100],\n",
    "                        [73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2e0ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86e74178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7d18466",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05baa99f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 hypothesis: tensor([-57.8508, -67.2810]) Cost: 52592.609375\n",
      "Epoch    0/20 Batch 2/3 hypothesis: tensor([29.4728, 37.8162]) Cost: 17162.720703\n",
      "Epoch    0/20 Batch 3/3 hypothesis: 101.76319122314453 Cost: 8880.576172\n",
      "Epoch    1/20 Batch 1/3 hypothesis: tensor([143.2189, 146.7879]) Cost: 1406.505615\n",
      "Epoch    1/20 Batch 2/3 hypothesis: tensor([135.4215, 176.0777]) Cost: 335.873413\n",
      "Epoch    1/20 Batch 3/3 hypothesis: 133.53038024902344 Cost: 71.734459\n",
      "Epoch    2/20 Batch 1/3 hypothesis: tensor([177.4618, 188.9932]) Cost: 52.959854\n",
      "Epoch    2/20 Batch 2/3 hypothesis: tensor([177.1528, 148.5720]) Cost: 9.929075\n",
      "Epoch    2/20 Batch 3/3 hypothesis: 139.9553985595703 Cost: 4.180395\n",
      "Epoch    3/20 Batch 1/3 hypothesis: tensor([150.3999, 179.3174]) Cost: 1.513111\n",
      "Epoch    3/20 Batch 2/3 hypothesis: tensor([183.9289, 140.9340]) Cost: 1.141822\n",
      "Epoch    3/20 Batch 3/3 hypothesis: 196.43800354003906 Cost: 0.191847\n",
      "Epoch    4/20 Batch 1/3 hypothesis: tensor([196.1855, 180.0444]) Cost: 0.018199\n",
      "Epoch    4/20 Batch 2/3 hypothesis: tensor([150.9633, 141.0684]) Cost: 0.971289\n",
      "Epoch    4/20 Batch 3/3 hypothesis: 184.49850463867188 Cost: 0.251498\n",
      "Epoch    5/20 Batch 1/3 hypothesis: tensor([184.7497, 196.8120]) Cost: 0.361040\n",
      "Epoch    5/20 Batch 2/3 hypothesis: tensor([151.3702, 141.4419]) Cost: 0.354040\n",
      "Epoch    5/20 Batch 3/3 hypothesis: 180.7008819580078 Cost: 0.491236\n",
      "Epoch    6/20 Batch 1/3 hypothesis: tensor([141.3596, 180.3602]) Cost: 0.269935\n",
      "Epoch    6/20 Batch 2/3 hypothesis: tensor([184.5199, 196.5651]) Cost: 0.274933\n",
      "Epoch    6/20 Batch 3/3 hypothesis: 151.281494140625 Cost: 0.516251\n",
      "Epoch    7/20 Batch 1/3 hypothesis: tensor([151.5309, 141.5892]) Cost: 0.194418\n",
      "Epoch    7/20 Batch 2/3 hypothesis: tensor([197.0413, 180.8304]) Cost: 0.886977\n",
      "Epoch    7/20 Batch 3/3 hypothesis: 184.47906494140625 Cost: 0.271373\n",
      "Epoch    8/20 Batch 1/3 hypothesis: tensor([151.4910, 184.7400]) Cost: 0.163329\n",
      "Epoch    8/20 Batch 2/3 hypothesis: tensor([196.9848, 141.6836]) Cost: 0.534935\n",
      "Epoch    8/20 Batch 3/3 hypothesis: 180.57740783691406 Cost: 0.333400\n",
      "Epoch    9/20 Batch 1/3 hypothesis: tensor([151.2265, 180.2968]) Cost: 0.343205\n",
      "Epoch    9/20 Batch 2/3 hypothesis: tensor([184.5101, 141.3778]) Cost: 0.313565\n",
      "Epoch    9/20 Batch 3/3 hypothesis: 196.8134307861328 Cost: 0.661670\n",
      "Epoch   10/20 Batch 1/3 hypothesis: tensor([196.3446, 184.3148]) Cost: 0.294138\n",
      "Epoch   10/20 Batch 2/3 hypothesis: tensor([151.2024, 141.2895]) Cost: 0.570523\n",
      "Epoch   10/20 Batch 3/3 hypothesis: 180.5658416748047 Cost: 0.320177\n",
      "Epoch   11/20 Batch 1/3 hypothesis: tensor([141.3068, 180.2908]) Cost: 0.282546\n",
      "Epoch   11/20 Batch 2/3 hypothesis: tensor([196.5185, 151.2714]) Cost: 0.399842\n",
      "Epoch   11/20 Batch 3/3 hypothesis: 184.48960876464844 Cost: 0.260499\n",
      "Epoch   12/20 Batch 1/3 hypothesis: tensor([151.4944, 180.6145]) Cost: 0.316630\n",
      "Epoch   12/20 Batch 2/3 hypothesis: tensor([184.6989, 141.5223]) Cost: 0.159427\n",
      "Epoch   12/20 Batch 3/3 hypothesis: 196.93508911132812 Cost: 0.874392\n",
      "Epoch   13/20 Batch 1/3 hypothesis: tensor([184.3634, 180.2381]) Cost: 0.230967\n",
      "Epoch   13/20 Batch 2/3 hypothesis: tensor([196.5040, 151.2600]) Cost: 0.400841\n",
      "Epoch   13/20 Batch 3/3 hypothesis: 141.35757446289062 Cost: 0.412711\n",
      "Epoch   14/20 Batch 1/3 hypothesis: tensor([151.4795, 184.7282]) Cost: 0.172367\n",
      "Epoch   14/20 Batch 2/3 hypothesis: tensor([141.6794, 180.7712]) Cost: 0.348737\n",
      "Epoch   14/20 Batch 3/3 hypothesis: 196.8380889892578 Cost: 0.702393\n",
      "Epoch   15/20 Batch 1/3 hypothesis: tensor([151.1443, 196.3550]) Cost: 0.429094\n",
      "Epoch   15/20 Batch 2/3 hypothesis: tensor([184.4083, 141.3010]) Cost: 0.419383\n",
      "Epoch   15/20 Batch 3/3 hypothesis: 180.5595245361328 Cost: 0.313068\n",
      "Epoch   16/20 Batch 1/3 hypothesis: tensor([184.4141, 151.2177]) Cost: 0.477589\n",
      "Epoch   16/20 Batch 2/3 hypothesis: tensor([141.5416, 196.7820]) Cost: 0.410811\n",
      "Epoch   16/20 Batch 3/3 hypothesis: 180.4718475341797 Cost: 0.222640\n",
      "Epoch   17/20 Batch 1/3 hypothesis: tensor([184.3685, 196.4007]) Cost: 0.279674\n",
      "Epoch   17/20 Batch 2/3 hypothesis: tensor([180.2921, 141.3095]) Cost: 0.281095\n",
      "Epoch   17/20 Batch 3/3 hypothesis: 151.27096557617188 Cost: 0.531491\n",
      "Epoch   18/20 Batch 1/3 hypothesis: tensor([184.7823, 180.6503]) Cost: 0.235144\n",
      "Epoch   18/20 Batch 2/3 hypothesis: tensor([151.4358, 141.5058]) Cost: 0.281267\n",
      "Epoch   18/20 Batch 3/3 hypothesis: 196.95840454101562 Cost: 0.918539\n",
      "Epoch   19/20 Batch 1/3 hypothesis: tensor([196.4060, 141.2749]) Cost: 0.345313\n",
      "Epoch   19/20 Batch 2/3 hypothesis: tensor([151.2080, 184.4031]) Cost: 0.491770\n",
      "Epoch   19/20 Batch 3/3 hypothesis: 180.58612060546875 Cost: 0.343537\n",
      "Epoch   20/20 Batch 1/3 hypothesis: tensor([151.2290, 196.4645]) Cost: 0.405120\n",
      "Epoch   20/20 Batch 2/3 hypothesis: tensor([141.3439, 180.3366]) Cost: 0.271869\n",
      "Epoch   20/20 Batch 3/3 hypothesis: 184.5063934326172 Cost: 0.243647\n"
     ]
    }
   ],
   "source": [
    "model = MultivariateLinearRegressionModel()\n",
    "\n",
    "# optimizer ÏÑ§Ï†ï\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "        # H(x) Í≥ÑÏÇ∞\n",
    "        \n",
    "        hypothesis = model(x_train)\n",
    "    \n",
    "        # cost Í≥ÑÏÇ∞\n",
    "        cost = F.mse_loss(hypothesis , y_train)\n",
    "\n",
    "        # costÎ°ú H(x) Í∞úÏÑ†\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Batch {}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, batch_idx+1, len(dataloader), hypothesis.squeeze().detach(), \n",
    "            cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537a9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
