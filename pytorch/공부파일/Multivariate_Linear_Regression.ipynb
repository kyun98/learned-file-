{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8527829f",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba481e95",
   "metadata": {},
   "source": [
    "## 이론적 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d50dac2",
   "metadata": {},
   "source": [
    "다음과 같은 훈련 데이터가 있습니다.     \n",
    "앞서 배운 단순 선형 회귀와 다른 점은 독립 변수 의 개수가 이제 1개가 아닌 3개라는 점입니다.     \n",
    "3개의 퀴즈 점수로부터 최종 점수를 예측하는 모델을 만들어보겠습니다.   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23bde54d",
   "metadata": {},
   "source": [
    "<img src=./LinearRegression_01.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478ac8d",
   "metadata": {},
   "source": [
    "독립 변수$x$ 의 개수가 3개므로 이를 수식으로 표현하면 아래와 같습니다\n",
    "\n",
    "$ H(x) = x_1w_1 + x_2w_2 + x_3w_3 + b $   \n",
    "\n",
    "$ cost(W, b) = \\frac{1}{m} \\sum^m_{i=1} \\left( H(x^{(i)}) - y^{(i)} \\right)^2 $   \n",
    "\n",
    " - $H(x)$: 주어진 $x$ 값에 대해 예측을 어떻게 할 것인가 ?\n",
    " - $cost(W, b)$: $H(x)$ 가 $y$ 를 얼마나 잘 예측했는가 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5dbbf2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0352ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a6069f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x200ee7b2b70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85c18b7",
   "metadata": {},
   "source": [
    "## 1.3  Naive Data Representation\n",
    "\n",
    "$ H(x_1, x_2, x_3) = x_1w_1 + x_2w_2 + x_3w_3 + b $ \n",
    "\n",
    "위의 식을 보면 이번에는 단순 선형 회귀와 다르게  𝑥  의 개수가 3개입니다. 그러니까 를 3개 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4adef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x1_train = torch.FloatTensor([[73],\n",
    "                              [93],\n",
    "                              [89],\n",
    "                              [96],\n",
    "                              [73]])\n",
    "x2_train = torch.FloatTensor([[80],\n",
    "                              [88],\n",
    "                              [91],\n",
    "                              [98],\n",
    "                              [66]])\n",
    "x3_train = torch.FloatTensor([[75],\n",
    "                              [93],\n",
    "                              [90],\n",
    "                              [100],\n",
    "                              [70]])\n",
    "y_train = torch.FloatTensor([[152],\n",
    "                             [185],\n",
    "                             [180],\n",
    "                             [196],\n",
    "                             [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d9771",
   "metadata": {},
   "source": [
    "이제 가중치 $W$ 와 편향 $b$ 를 선언합니다. 가중치 $W$ 도 3개 선언해주어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "267729bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 w와 편향 b 초기화\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4fce492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 : 0.0\n",
      "w2 : 0.0\n",
      "w2 : 0.0\n",
      "b  : 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"w1 :\", w1.item())\n",
    "print(\"w2 :\", w2.item())\n",
    "print(\"w2 :\", w2.item())\n",
    "print(\"b  :\", b.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "656e3bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
      "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563634\n",
      "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497607\n",
      "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435026\n",
      "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375730\n",
      "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319511\n",
      "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n",
      "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215696\n",
      "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167818\n",
      "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079378\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e11e0",
   "metadata": {},
   "source": [
    "위의 경우 가설을 선언하는 부분인 hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b에서도     \n",
    "x_train의 개수만큼 w와 곱해주도록 작성해준 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "810a9a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152.454\n"
     ]
    }
   ],
   "source": [
    "pred = (73*0.718 +  80* 0.613 +  75 * 0.680) \n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd87cdb",
   "metadata": {},
   "source": [
    "$H(X) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3}$\n",
    "\n",
    "위 식은 아래와 같이 두 벡터의 내적으로 표현할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734e8a1",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{pmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "w_3 \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "x_1w_1 + x_2w_2 + x_3w_3\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb5a75",
   "metadata": {},
   "source": [
    "두 벡터를 각각 $X$와 $W$로 표현한다면, 가설은 다음과 같습니다.\n",
    "\n",
    "$ H(X) = XW $\n",
    "\n",
    "$x$의 개수가 3개였음에도 이제는 $X$와 $W$라는 두 개의 변수로 표현된 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb32359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152],\n",
    "                             [185],\n",
    "                             [180],\n",
    "                             [196],\n",
    "                             [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ade3b",
   "metadata": {},
   "source": [
    "이는 독립 변수 $x$ 들의 수가 (샘플의 수 × 특성의 수) = 15개임을 의미합니다. 독립 변수 $x$들을 (샘플의 수 × 특성의 수)의 크기를 가지는 하나의 행렬로 표현해봅시다. 그리고 이 행렬을 $X$라고 하겠습니다.\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "    \\begin{array}{c}\n",
    "      x_{11}\\ x_{12}\\ x_{13}\\ \\\\\n",
    "      x_{21}\\ x_{22}\\ x_{23}\\ \\\\\n",
    "      x_{31}\\ x_{32}\\ x_{33}\\ \\\\\n",
    "      x_{41}\\ x_{42}\\ x_{43}\\ \\\\\n",
    "      x_{51}\\ x_{52}\\ x_{53}\\ \\\\\n",
    "    \\end{array}\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67259c91",
   "metadata": {},
   "source": [
    "그리고 여기에 가중치 $w1, w2, w3$ 을 원소로 하는 벡터를 $X$ 라 하고 이를 곱해보겠습니다.\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "   \\begin{array}{c}\n",
    "     x_{11}\\ x_{12}\\ x_{13}\\ \\\\\n",
    "     x_{21}\\ x_{22}\\ x_{23}\\ \\\\\n",
    "     x_{31}\\ x_{32}\\ x_{33}\\ \\\\\n",
    "     x_{41}\\ x_{42}\\ x_{43}\\ \\\\\n",
    "     x_{51}\\ x_{52}\\ x_{53}\\ \\\\\n",
    "   \\end{array}\n",
    "\\end{pmatrix}\n",
    "$\n",
    "$\\begin{pmatrix}\n",
    "    \\begin{array}{c}\n",
    "      w_{1} \\\\\n",
    "      w_{2} \\\\\n",
    "      w_{3} \\\\\n",
    "    \\end{array}\n",
    "\\end{pmatrix}$\n",
    "\\ =\n",
    "$\\begin{pmatrix}\n",
    "    \\begin{array}{c}\n",
    "      x_{11}w_{1}+ x_{12}w_{2}+ x_{13}w_{3}\\ \\\\\n",
    "      x_{21}w_{1}+ x_{22}w_{2}+ x_{23}w_{3}\\ \\\\\n",
    "      x_{31}w_{1}+ x_{32}w_{2}+ x_{33}w_{3}\\ \\\\\n",
    "      x_{41}w_{1}+ x_{42}w_{2}+ x_{43}w_{3}\\ \\\\\n",
    "      x_{51}w_{1}+ x_{52}w_{2}+ x_{53}w_{3}\\ \\\\\n",
    "    \\end{array}\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c270b",
   "metadata": {},
   "source": [
    "위의 식은 결과적으로 다음과 같습니다.   \n",
    "   \n",
    "$H(X) = XW$\n",
    "\n",
    "이 가설에 각 샘플에 더해지는 편향 $b$ 를 추가해봅시다. 샘플 수만큼의 차원을 가지는 편향 벡터$B$ 를 만들어 더합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b94de",
   "metadata": {},
   "source": [
    "$\\begin{pmatrix}\n",
    "    \\begin{array}{c}\n",
    "      x_{11}\\ x_{12}\\ x_{13}\\ \\\\\n",
    "      x_{21}\\ x_{22}\\ x_{23}\\ \\\\\n",
    "      x_{31}\\ x_{32}\\ x_{33}\\ \\\\\n",
    "      x_{41}\\ x_{42}\\ x_{43}\\ \\\\\n",
    "      x_{51}\\ x_{52}\\ x_{53}\\ \\\\\n",
    "    \\end{array}\n",
    "\\end{pmatrix}$\n",
    "$\\begin{pmatrix}\n",
    "    \\begin{array}{c}\n",
    "      w_{1} \\\\\n",
    "      w_{2} \\\\\n",
    "      w_{3} \\\\\n",
    "    \\end{array}\n",
    "\\end{pmatrix}$\n",
    "$+$\n",
    "$\\begin{pmatrix}\n",
    "    \\begin{array}{c}\n",
    "      b \\\\\n",
    "      b \\\\\n",
    "      b \\\\\n",
    "      b \\\\\n",
    "      b \\\\\n",
    "    \\end{array}\n",
    "\\end{pmatrix}$\n",
    "$\\ =$\n",
    "$\\begin{pmatrix}\n",
    "    \\begin{array}{c}\n",
    "      x_{11}w_{1}+ x_{12}w_{2}+ x_{13}w_{3} + b\\ \\\\\n",
    "      x_{21}w_{1}+ x_{22}w_{2}+ x_{23}w_{3} + b\\ \\\\\n",
    "      x_{31}w_{1}+ x_{32}w_{2}+ x_{33}w_{3} + b\\ \\\\\n",
    "      x_{41}w_{1}+ x_{42}w_{2}+ x_{43}w_{3} + b\\ \\\\\n",
    "      x_{51}w_{1}+ x_{52}w_{2}+ x_{53}w_{3} + b\\ \\\\\n",
    "    \\end{array}\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc26f84c",
   "metadata": {},
   "source": [
    "위의 식은 결과적으로 다음과 같습니다.\n",
    "\n",
    "$H(X) = XW + B$\n",
    "\n",
    "결과적으로 전체 훈련 데이터의 가설 연산을 3개의 변수만으로 표현하였습니다.   \n",
    "이와 같이 벡터와 행렬 연산은 식을 간단하게 해줄 뿐만 아니라 다수의 샘플의 병렬 연산이므로 속도의 이점을 가집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf9d2742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdd1f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 w와 편향 b 초기화\n",
    "W = torch.zeros((3,1), requires_grad=True)\n",
    "\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f7c3677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/30 hypothesis: tensor([152.7939, 183.6813, 180.9665, 197.0692, 140.1102])  Cost: 1.603586\n",
      "Epoch    1/30 hypothesis: tensor([152.7935, 183.6816, 180.9664, 197.0691, 140.1106])  Cost: 1.602878\n",
      "Epoch    2/30 hypothesis: tensor([152.7931, 183.6819, 180.9663, 197.0690, 140.1110])  Cost: 1.602179\n",
      "Epoch    3/30 hypothesis: tensor([152.7926, 183.6822, 180.9661, 197.0689, 140.1114])  Cost: 1.601486\n",
      "Epoch    4/30 hypothesis: tensor([152.7922, 183.6826, 180.9660, 197.0688, 140.1118])  Cost: 1.600765\n",
      "Epoch    5/30 hypothesis: tensor([152.7918, 183.6828, 180.9659, 197.0686, 140.1122])  Cost: 1.600072\n",
      "Epoch    6/30 hypothesis: tensor([152.7914, 183.6831, 180.9657, 197.0685, 140.1126])  Cost: 1.599369\n",
      "Epoch    7/30 hypothesis: tensor([152.7909, 183.6834, 180.9656, 197.0684, 140.1130])  Cost: 1.598692\n",
      "Epoch    8/30 hypothesis: tensor([152.7905, 183.6837, 180.9655, 197.0683, 140.1134])  Cost: 1.597991\n",
      "Epoch    9/30 hypothesis: tensor([152.7900, 183.6840, 180.9653, 197.0682, 140.1138])  Cost: 1.597289\n",
      "Epoch   10/30 hypothesis: tensor([152.7896, 183.6843, 180.9652, 197.0681, 140.1143])  Cost: 1.596590\n",
      "Epoch   11/30 hypothesis: tensor([152.7892, 183.6846, 180.9651, 197.0679, 140.1147])  Cost: 1.595898\n",
      "Epoch   12/30 hypothesis: tensor([152.7888, 183.6849, 180.9650, 197.0678, 140.1151])  Cost: 1.595198\n",
      "Epoch   13/30 hypothesis: tensor([152.7883, 183.6852, 180.9648, 197.0677, 140.1155])  Cost: 1.594514\n",
      "Epoch   14/30 hypothesis: tensor([152.7879, 183.6855, 180.9647, 197.0676, 140.1159])  Cost: 1.593821\n",
      "Epoch   15/30 hypothesis: tensor([152.7875, 183.6858, 180.9646, 197.0675, 140.1163])  Cost: 1.593129\n",
      "Epoch   16/30 hypothesis: tensor([152.7870, 183.6861, 180.9644, 197.0674, 140.1167])  Cost: 1.592433\n",
      "Epoch   17/30 hypothesis: tensor([152.7866, 183.6864, 180.9643, 197.0673, 140.1171])  Cost: 1.591737\n",
      "Epoch   18/30 hypothesis: tensor([152.7862, 183.6867, 180.9642, 197.0672, 140.1175])  Cost: 1.591038\n",
      "Epoch   19/30 hypothesis: tensor([152.7858, 183.6870, 180.9641, 197.0670, 140.1179])  Cost: 1.590336\n",
      "Epoch   20/30 hypothesis: tensor([152.7853, 183.6873, 180.9639, 197.0669, 140.1183])  Cost: 1.589665\n",
      "Epoch   21/30 hypothesis: tensor([152.7849, 183.6876, 180.9638, 197.0668, 140.1187])  Cost: 1.588963\n",
      "Epoch   22/30 hypothesis: tensor([152.7845, 183.6879, 180.9637, 197.0667, 140.1191])  Cost: 1.588273\n",
      "Epoch   23/30 hypothesis: tensor([152.7840, 183.6882, 180.9635, 197.0666, 140.1195])  Cost: 1.587576\n",
      "Epoch   24/30 hypothesis: tensor([152.7836, 183.6885, 180.9634, 197.0665, 140.1199])  Cost: 1.586890\n",
      "Epoch   25/30 hypothesis: tensor([152.7832, 183.6888, 180.9633, 197.0664, 140.1203])  Cost: 1.586196\n",
      "Epoch   26/30 hypothesis: tensor([152.7828, 183.6890, 180.9632, 197.0663, 140.1207])  Cost: 1.585522\n",
      "Epoch   27/30 hypothesis: tensor([152.7823, 183.6893, 180.9630, 197.0661, 140.1211])  Cost: 1.584821\n",
      "Epoch   28/30 hypothesis: tensor([152.7819, 183.6896, 180.9629, 197.0660, 140.1215])  Cost: 1.584131\n",
      "Epoch   29/30 hypothesis: tensor([152.7815, 183.6899, 180.9628, 197.0659, 140.1219])  Cost: 1.583444\n",
      "Epoch   30/30 hypothesis: tensor([152.7810, 183.6902, 180.9626, 197.0658, 140.1223])  Cost: 1.582750\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 30\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} hypothesis: {}  Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b58811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1],\n",
    "                             [2],\n",
    "                             [3],\n",
    "                             [4],\n",
    "                             [5]])\n",
    "y_train = torch.FloatTensor([[2],\n",
    "                             [4],\n",
    "                             [6],\n",
    "                             [8],\n",
    "                             [10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e751a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8922732e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 hypothesis: tensor([-0.0245,  0.3408,  0.7060,  1.0712,  1.4364])  Cost: 33.371571\n",
      "Epoch    1/100 hypothesis: tensor([0.4644, 1.2127, 1.9610, 2.7092, 3.4575])  Cost: 19.447556\n",
      "Epoch    2/100 hypothesis: tensor([0.8376, 1.8783, 2.9190, 3.9596, 5.0003])  Cost: 11.333327\n",
      "Epoch    3/100 hypothesis: tensor([1.1225, 2.3864, 3.6503, 4.9142, 6.1781])  Cost: 6.604755\n",
      "Epoch    4/100 hypothesis: tensor([1.3399, 2.7742, 4.2086, 5.6429, 7.0772])  Cost: 3.849176\n",
      "Epoch    5/100 hypothesis: tensor([1.5058, 3.0703, 4.6347, 6.1992, 7.7636])  Cost: 2.243360\n",
      "Epoch    6/100 hypothesis: tensor([1.6325, 3.2963, 4.9601, 6.6238, 8.2876])  Cost: 1.307568\n",
      "Epoch    7/100 hypothesis: tensor([1.7291, 3.4688, 5.2084, 6.9480, 8.6876])  Cost: 0.762231\n",
      "Epoch    8/100 hypothesis: tensor([1.8029, 3.6004, 5.3980, 7.1955, 8.9930])  Cost: 0.444436\n",
      "Epoch    9/100 hypothesis: tensor([1.8591, 3.7009, 5.5427, 7.3844, 9.2262])  Cost: 0.259238\n",
      "Epoch   10/100 hypothesis: tensor([1.9021, 3.7776, 5.6531, 7.5286, 9.4042])  Cost: 0.151313\n",
      "Epoch   11/100 hypothesis: tensor([1.9348, 3.8361, 5.7374, 7.6388, 9.5401])  Cost: 0.088417\n",
      "Epoch   12/100 hypothesis: tensor([1.9597, 3.8808, 5.8018, 7.7228, 9.6438])  Cost: 0.051763\n",
      "Epoch   13/100 hypothesis: tensor([1.9788, 3.9148, 5.8509, 7.7870, 9.7231])  Cost: 0.030402\n",
      "Epoch   14/100 hypothesis: tensor([1.9932, 3.9408, 5.8884, 7.8360, 9.7836])  Cost: 0.017952\n",
      "Epoch   15/100 hypothesis: tensor([2.0043, 3.9606, 5.9170, 7.8734, 9.8297])  Cost: 0.010695\n",
      "Epoch   16/100 hypothesis: tensor([2.0126, 3.9757, 5.9388, 7.9019, 9.8650])  Cost: 0.006464\n",
      "Epoch   17/100 hypothesis: tensor([2.0190, 3.9873, 5.9555, 7.9237, 9.8920])  Cost: 0.003997\n",
      "Epoch   18/100 hypothesis: tensor([2.0238, 3.9960, 5.9682, 7.9404, 9.9126])  Cost: 0.002558\n",
      "Epoch   19/100 hypothesis: tensor([2.0275, 4.0027, 5.9779, 7.9531, 9.9283])  Cost: 0.001718\n",
      "Epoch   20/100 hypothesis: tensor([2.0303, 4.0078, 5.9853, 7.9628, 9.9403])  Cost: 0.001227\n",
      "Epoch   21/100 hypothesis: tensor([2.0323, 4.0116, 5.9909, 7.9702, 9.9495])  Cost: 0.000939\n",
      "Epoch   22/100 hypothesis: tensor([2.0339, 4.0146, 5.9952, 7.9759, 9.9566])  Cost: 0.000770\n",
      "Epoch   23/100 hypothesis: tensor([2.0350, 4.0168, 5.9985, 7.9802, 9.9620])  Cost: 0.000670\n",
      "Epoch   24/100 hypothesis: tensor([2.0359, 4.0184, 6.0010, 7.9835, 9.9661])  Cost: 0.000610\n",
      "Epoch   25/100 hypothesis: tensor([2.0365, 4.0197, 6.0029, 7.9861, 9.9693])  Cost: 0.000574\n",
      "Epoch   26/100 hypothesis: tensor([2.0370, 4.0206, 6.0043, 7.9880, 9.9717])  Cost: 0.000551\n",
      "Epoch   27/100 hypothesis: tensor([2.0373, 4.0213, 6.0054, 7.9895, 9.9736])  Cost: 0.000536\n",
      "Epoch   28/100 hypothesis: tensor([2.0375, 4.0219, 6.0062, 7.9906, 9.9750])  Cost: 0.000526\n",
      "Epoch   29/100 hypothesis: tensor([2.0376, 4.0222, 6.0069, 7.9915, 9.9762])  Cost: 0.000519\n",
      "Epoch   30/100 hypothesis: tensor([2.0377, 4.0225, 6.0073, 7.9922, 9.9770])  Cost: 0.000513\n",
      "Epoch   31/100 hypothesis: tensor([2.0377, 4.0227, 6.0077, 7.9927, 9.9777])  Cost: 0.000509\n",
      "Epoch   32/100 hypothesis: tensor([2.0377, 4.0228, 6.0080, 7.9931, 9.9782])  Cost: 0.000504\n",
      "Epoch   33/100 hypothesis: tensor([2.0376, 4.0229, 6.0081, 7.9934, 9.9787])  Cost: 0.000501\n",
      "Epoch   34/100 hypothesis: tensor([2.0376, 4.0229, 6.0083, 7.9936, 9.9790])  Cost: 0.000497\n",
      "Epoch   35/100 hypothesis: tensor([2.0375, 4.0229, 6.0084, 7.9938, 9.9793])  Cost: 0.000493\n",
      "Epoch   36/100 hypothesis: tensor([2.0374, 4.0229, 6.0085, 7.9940, 9.9795])  Cost: 0.000490\n",
      "Epoch   37/100 hypothesis: tensor([2.0373, 4.0229, 6.0085, 7.9941, 9.9797])  Cost: 0.000487\n",
      "Epoch   38/100 hypothesis: tensor([2.0372, 4.0229, 6.0085, 7.9942, 9.9799])  Cost: 0.000483\n",
      "Epoch   39/100 hypothesis: tensor([2.0371, 4.0228, 6.0085, 7.9943, 9.9800])  Cost: 0.000480\n",
      "Epoch   40/100 hypothesis: tensor([2.0370, 4.0228, 6.0085, 7.9943, 9.9801])  Cost: 0.000477\n",
      "Epoch   41/100 hypothesis: tensor([2.0368, 4.0227, 6.0085, 7.9944, 9.9802])  Cost: 0.000474\n",
      "Epoch   42/100 hypothesis: tensor([2.0367, 4.0226, 6.0085, 7.9944, 9.9803])  Cost: 0.000470\n",
      "Epoch   43/100 hypothesis: tensor([2.0366, 4.0226, 6.0085, 7.9945, 9.9804])  Cost: 0.000467\n",
      "Epoch   44/100 hypothesis: tensor([2.0365, 4.0225, 6.0085, 7.9945, 9.9805])  Cost: 0.000464\n",
      "Epoch   45/100 hypothesis: tensor([2.0364, 4.0224, 6.0085, 7.9945, 9.9806])  Cost: 0.000461\n",
      "Epoch   46/100 hypothesis: tensor([2.0363, 4.0224, 6.0085, 7.9946, 9.9807])  Cost: 0.000458\n",
      "Epoch   47/100 hypothesis: tensor([2.0361, 4.0223, 6.0084, 7.9946, 9.9807])  Cost: 0.000455\n",
      "Epoch   48/100 hypothesis: tensor([2.0360, 4.0222, 6.0084, 7.9946, 9.9808])  Cost: 0.000452\n",
      "Epoch   49/100 hypothesis: tensor([2.0359, 4.0221, 6.0084, 7.9946, 9.9809])  Cost: 0.000449\n",
      "Epoch   50/100 hypothesis: tensor([2.0358, 4.0221, 6.0084, 7.9947, 9.9809])  Cost: 0.000446\n",
      "Epoch   51/100 hypothesis: tensor([2.0357, 4.0220, 6.0083, 7.9947, 9.9810])  Cost: 0.000443\n",
      "Epoch   52/100 hypothesis: tensor([2.0355, 4.0219, 6.0083, 7.9947, 9.9811])  Cost: 0.000440\n",
      "Epoch   53/100 hypothesis: tensor([2.0354, 4.0218, 6.0083, 7.9947, 9.9811])  Cost: 0.000437\n",
      "Epoch   54/100 hypothesis: tensor([2.0353, 4.0218, 6.0082, 7.9947, 9.9812])  Cost: 0.000434\n",
      "Epoch   55/100 hypothesis: tensor([2.0352, 4.0217, 6.0082, 7.9947, 9.9813])  Cost: 0.000431\n",
      "Epoch   56/100 hypothesis: tensor([2.0351, 4.0216, 6.0082, 7.9948, 9.9813])  Cost: 0.000428\n",
      "Epoch   57/100 hypothesis: tensor([2.0349, 4.0216, 6.0082, 7.9948, 9.9814])  Cost: 0.000425\n",
      "Epoch   58/100 hypothesis: tensor([2.0348, 4.0215, 6.0081, 7.9948, 9.9815])  Cost: 0.000422\n",
      "Epoch   59/100 hypothesis: tensor([2.0347, 4.0214, 6.0081, 7.9948, 9.9815])  Cost: 0.000419\n",
      "Epoch   60/100 hypothesis: tensor([2.0346, 4.0213, 6.0081, 7.9948, 9.9816])  Cost: 0.000416\n",
      "Epoch   61/100 hypothesis: tensor([2.0345, 4.0213, 6.0081, 7.9949, 9.9817])  Cost: 0.000414\n",
      "Epoch   62/100 hypothesis: tensor([2.0343, 4.0212, 6.0080, 7.9949, 9.9817])  Cost: 0.000411\n",
      "Epoch   63/100 hypothesis: tensor([2.0342, 4.0211, 6.0080, 7.9949, 9.9818])  Cost: 0.000408\n",
      "Epoch   64/100 hypothesis: tensor([2.0341, 4.0210, 6.0080, 7.9949, 9.9818])  Cost: 0.000405\n",
      "Epoch   65/100 hypothesis: tensor([2.0340, 4.0210, 6.0079, 7.9949, 9.9819])  Cost: 0.000403\n",
      "Epoch   66/100 hypothesis: tensor([2.0339, 4.0209, 6.0079, 7.9949, 9.9820])  Cost: 0.000400\n",
      "Epoch   67/100 hypothesis: tensor([2.0338, 4.0208, 6.0079, 7.9950, 9.9820])  Cost: 0.000397\n",
      "Epoch   68/100 hypothesis: tensor([2.0337, 4.0208, 6.0079, 7.9950, 9.9821])  Cost: 0.000394\n",
      "Epoch   69/100 hypothesis: tensor([2.0335, 4.0207, 6.0078, 7.9950, 9.9821])  Cost: 0.000392\n",
      "Epoch   70/100 hypothesis: tensor([2.0334, 4.0206, 6.0078, 7.9950, 9.9822])  Cost: 0.000389\n",
      "Epoch   71/100 hypothesis: tensor([2.0333, 4.0206, 6.0078, 7.9950, 9.9823])  Cost: 0.000387\n",
      "Epoch   72/100 hypothesis: tensor([2.0332, 4.0205, 6.0078, 7.9950, 9.9823])  Cost: 0.000384\n",
      "Epoch   73/100 hypothesis: tensor([2.0331, 4.0204, 6.0077, 7.9951, 9.9824])  Cost: 0.000381\n",
      "Epoch   74/100 hypothesis: tensor([2.0330, 4.0203, 6.0077, 7.9951, 9.9824])  Cost: 0.000379\n",
      "Epoch   75/100 hypothesis: tensor([2.0329, 4.0203, 6.0077, 7.9951, 9.9825])  Cost: 0.000376\n",
      "Epoch   76/100 hypothesis: tensor([2.0328, 4.0202, 6.0077, 7.9951, 9.9826])  Cost: 0.000374\n",
      "Epoch   77/100 hypothesis: tensor([2.0326, 4.0201, 6.0076, 7.9951, 9.9826])  Cost: 0.000371\n",
      "Epoch   78/100 hypothesis: tensor([2.0325, 4.0201, 6.0076, 7.9951, 9.9827])  Cost: 0.000369\n",
      "Epoch   79/100 hypothesis: tensor([2.0324, 4.0200, 6.0076, 7.9952, 9.9827])  Cost: 0.000366\n",
      "Epoch   80/100 hypothesis: tensor([2.0323, 4.0199, 6.0076, 7.9952, 9.9828])  Cost: 0.000364\n",
      "Epoch   81/100 hypothesis: tensor([2.0322, 4.0199, 6.0075, 7.9952, 9.9829])  Cost: 0.000361\n",
      "Epoch   82/100 hypothesis: tensor([2.0321, 4.0198, 6.0075, 7.9952, 9.9829])  Cost: 0.000359\n",
      "Epoch   83/100 hypothesis: tensor([2.0320, 4.0197, 6.0075, 7.9952, 9.9830])  Cost: 0.000356\n",
      "Epoch   84/100 hypothesis: tensor([2.0319, 4.0197, 6.0075, 7.9952, 9.9830])  Cost: 0.000354\n",
      "Epoch   85/100 hypothesis: tensor([2.0318, 4.0196, 6.0074, 7.9953, 9.9831])  Cost: 0.000352\n",
      "Epoch   86/100 hypothesis: tensor([2.0317, 4.0195, 6.0074, 7.9953, 9.9831])  Cost: 0.000349\n",
      "Epoch   87/100 hypothesis: tensor([2.0316, 4.0195, 6.0074, 7.9953, 9.9832])  Cost: 0.000347\n",
      "Epoch   88/100 hypothesis: tensor([2.0315, 4.0194, 6.0074, 7.9953, 9.9833])  Cost: 0.000344\n",
      "Epoch   89/100 hypothesis: tensor([2.0313, 4.0193, 6.0073, 7.9953, 9.9833])  Cost: 0.000342\n",
      "Epoch   90/100 hypothesis: tensor([2.0312, 4.0193, 6.0073, 7.9953, 9.9834])  Cost: 0.000340\n",
      "Epoch   91/100 hypothesis: tensor([2.0311, 4.0192, 6.0073, 7.9954, 9.9834])  Cost: 0.000338\n",
      "Epoch   92/100 hypothesis: tensor([2.0310, 4.0191, 6.0073, 7.9954, 9.9835])  Cost: 0.000335\n",
      "Epoch   93/100 hypothesis: tensor([2.0309, 4.0191, 6.0072, 7.9954, 9.9835])  Cost: 0.000333\n",
      "Epoch   94/100 hypothesis: tensor([2.0308, 4.0190, 6.0072, 7.9954, 9.9836])  Cost: 0.000331\n",
      "Epoch   95/100 hypothesis: tensor([2.0307, 4.0189, 6.0072, 7.9954, 9.9836])  Cost: 0.000329\n",
      "Epoch   96/100 hypothesis: tensor([2.0306, 4.0189, 6.0072, 7.9954, 9.9837])  Cost: 0.000326\n",
      "Epoch   97/100 hypothesis: tensor([2.0305, 4.0188, 6.0071, 7.9954, 9.9838])  Cost: 0.000324\n",
      "Epoch   98/100 hypothesis: tensor([2.0304, 4.0188, 6.0071, 7.9955, 9.9838])  Cost: 0.000322\n",
      "Epoch   99/100 hypothesis: tensor([2.0303, 4.0187, 6.0071, 7.9955, 9.9839])  Cost: 0.000320\n",
      "Epoch  100/100 hypothesis: tensor([2.0302, 4.0186, 6.0071, 7.9955, 9.9839])  Cost: 0.000318\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 100\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(hypothesis , y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} hypothesis: {}  Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8815b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152],\n",
    "                             [185],\n",
    "                             [180],\n",
    "                             [196],\n",
    "                             [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68b7c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "019dca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([-6.7933, -4.8968, -6.5155, -7.3361, -2.6660])  Cost: 31667.599609\n",
      "Epoch    1/20 hypothesis: tensor([62.7036, 78.6330, 75.7880, 82.2903, 61.0462])  Cost: 9926.266602\n",
      "Epoch    2/20 hypothesis: tensor([101.6124, 125.3983, 121.8667, 132.4688,  96.7163])  Cost: 3111.513916\n",
      "Epoch    3/20 hypothesis: tensor([123.3960, 151.5805, 147.6645, 160.5620, 116.6867])  Cost: 975.451355\n",
      "Epoch    4/20 hypothesis: tensor([135.5919, 166.2389, 162.1078, 176.2903, 127.8673])  Cost: 305.908539\n",
      "Epoch    5/20 hypothesis: tensor([142.4200, 174.4456, 170.1940, 185.0960, 134.1269])  Cost: 96.042496\n",
      "Epoch    6/20 hypothesis: tensor([146.2428, 179.0401, 174.7213, 190.0260, 137.6314])  Cost: 30.260748\n",
      "Epoch    7/20 hypothesis: tensor([148.3830, 181.6125, 177.2559, 192.7861, 139.5934])  Cost: 9.641701\n",
      "Epoch    8/20 hypothesis: tensor([149.5814, 183.0526, 178.6749, 194.3314, 140.6919])  Cost: 3.178671\n",
      "Epoch    9/20 hypothesis: tensor([150.2523, 183.8588, 179.4694, 195.1966, 141.3068])  Cost: 1.152871\n",
      "Epoch   10/20 hypothesis: tensor([150.6279, 184.3102, 179.9142, 195.6810, 141.6511])  Cost: 0.517863\n",
      "Epoch   11/20 hypothesis: tensor([150.8383, 184.5629, 180.1633, 195.9521, 141.8438])  Cost: 0.318801\n",
      "Epoch   12/20 hypothesis: tensor([150.9561, 184.7043, 180.3027, 196.1040, 141.9516])  Cost: 0.256388\n",
      "Epoch   13/20 hypothesis: tensor([151.0221, 184.7835, 180.3808, 196.1890, 142.0120])  Cost: 0.236821\n",
      "Epoch   14/20 hypothesis: tensor([151.0591, 184.8278, 180.4245, 196.2366, 142.0458])  Cost: 0.230660\n",
      "Epoch   15/20 hypothesis: tensor([151.0798, 184.8526, 180.4490, 196.2632, 142.0646])  Cost: 0.228719\n",
      "Epoch   16/20 hypothesis: tensor([151.0914, 184.8664, 180.4627, 196.2782, 142.0752])  Cost: 0.228095\n",
      "Epoch   17/20 hypothesis: tensor([151.0980, 184.8741, 180.4704, 196.2865, 142.0811])  Cost: 0.227880\n",
      "Epoch   18/20 hypothesis: tensor([151.1017, 184.8784, 180.4747, 196.2912, 142.0843])  Cost: 0.227799\n",
      "Epoch   19/20 hypothesis: tensor([151.1038, 184.8808, 180.4772, 196.2939, 142.0861])  Cost: 0.227762\n",
      "Epoch   20/20 hypothesis: tensor([151.1050, 184.8821, 180.4786, 196.2953, 142.0871])  Cost: 0.227732\n"
     ]
    }
   ],
   "source": [
    "model = MultivariateLinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(hypothesis , y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} hypothesis: {}  Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05baa99f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
