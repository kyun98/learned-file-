{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25일차\n",
    "# 시그모이드 함수 그리기(plt.plot(x, y))\n",
    "# 로지스틱회귀\n",
    "z = (x*w).sum() + b # x= data 행렬, w = 가중치 행렬\n",
    "# MSE : (y-pred_y)**2\n",
    "# crossentropy : Error = -np.log(y-pred_y)\n",
    "# 2차원 행렬에서의 계산 : (X*w).sum(axis=1)+b / pred_y = 1/(1+np.exp(-(X@w + b)))\n",
    "# MSE : ((y-pred_y)**2).mean()  / crossentropy : (-np.log(pred_y[0]) - np.log(1-pred_y[1]) - np.log(pred_y[2]))/3\n",
    "# 기울기 계산 : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c073651c",
   "metadata": {},
   "source": [
    "$$ sigmoid(x) = \\frac {1} {1 + e^{-x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741958ff",
   "metadata": {},
   "source": [
    "- 기울기 직접 계산\n",
    "- ![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ec0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26일차\n",
    "# 유방암 데이터\n",
    "# 정규화 : X_norm = ((X - X.mean(axis=0)) / X.std(axis=0))[:,:2]\n",
    "# 로지스틱스 회귀 : sklearn.linear_model.LogisticRegretion > coef_,intercept_이용해 시각화\n",
    "\n",
    "# 신경망\n",
    "w = np.random.randn(3,3) # 3*3행렬로 가중치 표현 (전레이어 노드, 현재 노드)\n",
    "b = np.random.randn(3) # 현재의 노드수만큼 바이어스 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd465a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 : 사람이 사용하는 언어(자연어)를 컴퓨터가 이해할 수 있는 언어(숫자)로 벡터화하는 것\n",
    "# 원핫 인코딩 : 주어진 텍스트를 숫자(벡터)로 변환해주는 것\n",
    "# 임베딩 > 원-핫 : 해당 임베딩 들어온 아이디에 맞는 단어를 원핫에서 플랙 온 시킴\n",
    "# 희소 표현 기반 임베딩, 횟수 기반 임베딩, 예측 기반 임베딩, 횟수/예측 기반 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c14265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder() # 벡터화\n",
    "onehot_encoder = preprocessing.OneHotEncoder() # 원핫 인코딩\n",
    "encoder.fit_transform(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40748917",
   "metadata": {},
   "source": [
    "# 카운터 벡터라이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc4b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원-핫 벡터들은 하나의 요소만 1 값을 갖고 나머지는 모두 0인(희소 벡터)가 됨\n",
    "# 원핫 인코딩의 맹점(희소성은 보장하지만) : 1. 하나의 요소만 1의 값을 갖고 나머지는 모두 0인 희소 벡터를 가짐\n",
    "# 내적해보면 직교(0) > 독립적인 관계로 존재하게 됨(단어끼리의 관계성이 없어짐)\n",
    "# 2. 차원의 저주 : 하나의 단어를 표현하는 데 너무 많아짐 차원이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c211e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 횟수 기반 임베딩\n",
    "# 한단어가 젠체 문서에서 얼마나 공통적으로 많이 등장하는지 나타내는 값(출현 빈도)\n",
    "#즉, 특정 단어가 나타난 문서의 개수로 이해\n",
    "\n",
    "# idf = log(n/(1+df)) = log(전체 문서 개수/(1+특정 단어 t가 포함된 문서 개수))\n",
    "# 카운트 벡터 : 토큰화 하며 각 단어의 출현 빈도수를 이용해 인코딩해 벡터로 만드는 방법 (횟수 기반 임베딩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92055e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_ # 빈도가 높을수록 높은 값으로 책정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271591da",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words = [\"and\", \"is\", \"please\", \"this\"]).fit(corpus) # 불용어 제거 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ca861",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.transform(['you will never get any chance']).toarray() # 배열 변환 : id 1부터 시작하는 배열(원-핫)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2109ea50",
   "metadata": {},
   "source": [
    "# tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF(문서 빈도)-IDF(역문서 빈도) > 문서빈도 : 한단어가 문서에서 얼마나 등장하는지/DF : 특정단어가 나타난 문서 갯수\n",
    "# 일반적인 단어(a, the)/빈도수 높지만 뜻이 없는/가중치를 낮추기 위해 df의 역수를 취해줌 > idf\n",
    "# 문장에 대한 유사도를 나타내줌 (퍼센테이지로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d5919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfidfVectorizer.fit_transform(doc) # 번환\n",
    "tfidf_vectorizer.vocabulary_ # 사전으로 반환\n",
    "tfidf_vectorizer.vocabulary_.items() # 사전 정렬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb7ad64",
   "metadata": {},
   "source": [
    "# word2vec (CBOW, Skip-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb70e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 기반 임베딩 : 특정 문맥에서 어떤 단어가 나올지 예측 (워드 투 벡터로 실습)\n",
    "# 문장에서 단어수만큼 벡터를 만듬\n",
    "# CBOW(counting bag of words) : 단어를 여러개 나열한 후 이와 관련된 단어를 추정 / 다음 등장할 단어를 예측\n",
    "# 은닉층의 크기는 입력 텍스트를 임베딩한 크기(N) > 차원이 7이고, N이 5라면 7 X 5행렬일것\n",
    "# 데이터셋을 메모리로 로딩하고 토큰화 적용 / 워드 투 벡터 - 코사인 유사도를 이용한 거리 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad1168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW 단어를 여러개 나열한 후 이와 관련된 단어를 추정\n",
    "from gensim.models import Word2Vec\n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5) # CBOW\n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5, sg = 1) # Skip-Gram\n",
    "# 옵션 : (데이터셋, 최소 빈도수 제한, 임베딩된 벡터 차원(N), 윈도우 크기, 스킵그램)\n",
    "model1.wv.similarity('peter', 'wendy') # 코사인 시밀러리티 측정\n",
    "# 중심 단어에서 주변 단어를 예측\n",
    "# 보통 입력단어 주변 k개의 단어를 문맥으로 보고 예측 모형을 만드는데 이 값을 윈도우 크기라고 함\n",
    "# skip-gram 특정 단어에서 문맥이 될 수 있는 단어를 예측 # 평소에 디폴트 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b1c9d",
   "metadata": {},
   "source": [
    "# 페스트 텍스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페스트 텍스트 : 인공 신경망을 이용해 학습을 한 후 모든 단어를 각 n-gram에 대해 임베딩 / 사전에 없는 단어시 > n-gram\n",
    "model = FastText('data_c10/peter.txt', vector_size=4, window=3, min_count=1, epochs=10)\n",
    "# 옵션 : 데이터셋, 임베딩의 크기, 고려할 앞 뒤 폭, 단어의 최소 빈도수 제한, 반복횟수\n",
    "model.wv.similarity('peter', 'wendy') # 코사인 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a173fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습 모델 사용\n",
    "from __future__ import print_function\n",
    "from gensim.models import KeyedVectors\n",
    "model_kr = KeyedVectors.load_word2vec_format('data_c10/wiki.ko.vec')\n",
    "model_kr.similar_by_word(...) # ...과 유사한단어 순위 10위\n",
    "model_kr.most_similar(positive=['동물', '육식동물'], negative=['사람']) # 유사도 +, - 방향 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d8e6da",
   "metadata": {},
   "source": [
    "# 글로브"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d7ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 횟수/예측 기반 임베딩\n",
    "# 글로브 : 글로벌 동시 발생 확률(통계 정보 + skip-gram)\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(Path/file, 파일을 워드투벡터 형태로 변환)\n",
    "KeyedVectors.load_word2vec_format(word2vec_glove_file).most_similar(positive = ['woman', 'king'], negative = ['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e7b6f6",
   "metadata": {},
   "source": [
    "# 트랜스포머 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e387a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트렌스포머 어텐션 : 인코더에서 벡터 변환 처리하고 모든 벡터 디코더로 보냄(다안보내면 기울기 소멸 문제)\n",
    "# 보내진 벡터중 중요한 부분만 소프트 맥스 함수를 이용해 높게 책정(필요한 부분만 집중) > 번역에 많이 쓰임\n",
    "# 인코더 : 셀프 어텐션 > 전방향 신경망\n",
    "# 디코더 : (셀프 어텐션 > 인코더-디코더 > 전방향 신경망)\n",
    "# seq2seq : 입력 시퀀스에서 출력 시퀀스를 만듬 > 품사판별보단 번역에 초점 / 입출력 시퀀스가 다를 수 있다(길이)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
